def create_adaptive_prompt(
    city_doc: str, 
    target_generation: str,
    persona_name: str,
    mbti: str = None,
    include_mbti: bool = False
) -> str:
    """í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ìƒ‰ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    persona_data = PERSONAS.get(persona_name)
    if not persona_data:
        return None

    # í˜ë¥´ì†Œë‚˜ì˜ ìƒ˜í”Œ ë¬¸ì¥ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìŠ¤íƒ€ì¼ì„ ì•”ì‹œì ìœ¼ë¡œ ì „ë‹¬
    import random
    sample_sentence = random.choice(persona_data['samples'])

    base_prompt = f'''
[ë°°ê²½ ì •ë³´]
- ë„ì‹œ ì •ë³´: {city_doc}
- íƒ€ê²Ÿ ì„¸ëŒ€: {target_generation}

[ì‘ì„± ì§€ì¹¨]
- ìœ„ ë°°ê²½ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì¤„ì˜ ê°•ë ¥í•œ ê´‘ê³  ì¹´í”¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ì¹´í”¼ëŠ” ë…ìì˜ ë§ˆìŒì„ ìš¸ë¦´ ìˆ˜ ìˆëŠ” ì§§ê³  ê°•ë ¬í•œ ë¬¸ì¥ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ê°ì •ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ëŠ” ì€ìœ ì™€ í•¨ì¶•ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
- í´ë¦¬ì…°ë‚˜ ì§„ë¶€í•œ í‘œí˜„ì„ í”¼í•˜ê³ , ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ ê´€ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
- ì´ëª¨ì§€ 1-2ê°œë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì•„ë˜ëŠ” ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ì…ë‹ˆë‹¤:
  "{sample_sentence}"
'''

    return base_prompt

def create_revision_prompt(original_copy: str, evaluation_result: dict) -> str:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‡´ê³  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    revision_prompt = f"""
ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ê´‘ê³  ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. ì•„ë˜ ê´‘ê³  ì¹´í”¼ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

[ì›ë³¸ ì¹´í”¼]
{original_copy}

[í˜„ì¬ í‰ê°€ ê²°ê³¼]
- ì´ì : {evaluation_result.get('score', 0)}ì 
- í‰ê°€ ì´ìœ : {evaluation_result.get('reason', 'í‰ê°€ ì—†ìŒ')}
- ì„¸ë¶€ ì ìˆ˜:
{chr(10).join([f'- {criterion}: {score}ì ' for criterion, score in zip(st.session_state.scoring_config.criteria, evaluation_result.get('detailed_scores', []))])}

[ê°œì„  ìš”êµ¬ì‚¬í•­]
1. ê° í‰ê°€ ê¸°ì¤€ì˜ ì ìˆ˜ë¥¼ ë¶„ì„í•˜ì—¬, ê°€ì¥ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ í•­ëª©ì„ ì¤‘ì ì ìœ¼ë¡œ ê°œì„ í•˜ì„¸ìš”.
2. ì›ë³¸ ì¹´í”¼ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ í†¤ì•¤ë§¤ë„ˆëŠ” ìœ ì§€í•˜ë©´ì„œ, ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ê°œì„ í•˜ì„¸ìš”:
   - ê°ì •ì  ê³µê°ë ¥: íƒ€ê²Ÿ ë…ìì˜ ê°ì •ì„ ë” ê°•í•˜ê²Œ ìê·¹í•˜ëŠ” í‘œí˜„ ì‚¬ìš©
   - ê²½í—˜ì˜ ìƒìƒí•¨: êµ¬ì²´ì ì´ê³  ê°ê°ì ì¸ í‘œí˜„ìœ¼ë¡œ ê²½í—˜ì„ ë” ìƒìƒí•˜ê²Œ ì „ë‹¬
   - ë…ìì™€ì˜ ì¡°í™”: íƒ€ê²Ÿ ì„¸ëŒ€ì˜ ì–¸ì–´ì™€ ê´€ì‹¬ì‚¬ë¥¼ ë” ì ê·¹ì ìœ¼ë¡œ ë°˜ì˜
   - ë¬¸í™”ì /ì§€ì—­ì  íŠ¹ì„±: ì§€ì—­ì˜ íŠ¹ìƒ‰ìˆëŠ” ìš”ì†Œë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í™œìš©

[ì œì•½ ì‚¬í•­]
- ë°˜ë“œì‹œ ê¸°ì¡´ í‰ê°€ ì ìˆ˜ë³´ë‹¤ ë†’ì€ í’ˆì§ˆì˜ ì¹´í”¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
- í˜•ì‹ì€ ë°˜ë“œì‹œ "**ì¹´í”¼**: (ë‚´ìš©)" í˜•íƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.
- ì„¤ëª…ë„ ë°˜ë“œì‹œ "**ì„¤ëª…**: (ë‚´ìš©)" í˜•íƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.

ê°œì„ ëœ ë²„ì „ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    return revision_prompt

def handle_revision_results(original_result: dict, revision_result: dict) -> dict:
    """í‡´ê³  ê²°ê³¼ ì²˜ë¦¬ - ì ìˆ˜ê°€ ë” ë†’ì€ ë²„ì „ì„ ì„ íƒ"""
    original_score = original_result.get('score', 0)
    revision_score = revision_result.get('score', 0)
    
    if revision_score > original_score:
        return revision_result, True  # ê°œì„ ë¨
    else:
        return original_result, False  # ì›ë³¸ ìœ ì§€

def generate_revision(original_copy: str, evaluation_result: dict, model_name: str) -> Dict:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‡´ê³ ëœ ë²„ì „ ìƒì„±"""
    revision_prompt = create_revision_prompt(original_copy, evaluation_result)
    return generate_copy(revision_prompt, model_name)  # ê¸°ì¡´ generate_copy í•¨ìˆ˜ í™œìš©


def generate_copy(prompt: str, model_name: str) -> Union[str, Dict]:
    """ê´‘ê³  ì¹´í”¼ ìƒì„±"""
    try:
        if model_name == "gpt":
            response = client.chat.completions.create(
                model=model_zoo[0],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000
            )
            return {
                "success": True,
                "content": response.choices[0].message.content.strip()
            }
            
        elif model_name == "gemini":
            try:
                response = gemini_model.generate_content(prompt)  # ë‹¨ìˆœí™”
                generated_text = response.text.strip()  # ë°”ë¡œ text ì¶”ì¶œ
                
                if generated_text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                    return {
                        "success": True,
                        "content": generated_text
                    }
                else:
                    return {
                        "success": False,
                        "content": "Geminiê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    }
                    
            except Exception as e:
                print(f"Gemini ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš©
                return {
                    "success": False,
                    "content": f"Gemini API ì˜¤ë¥˜: {str(e)}"
                }
            
        else:  # claude
            try:
                response = anthropic.messages.create(
                    model=model_zoo[2],
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=1000,
                    temperature=0.7
                )
                return {
                    "success": True,
                    "content": response.content[0].text.strip()
                }
            except Exception as e:
                return {
                    "success": False,
                    "content": f"Claude API ì˜¤ë¥˜: {str(e)}"
                }
                
    except Exception as e:
        return {
            "success": False,
            "content": f"ìƒì„± ì‹¤íŒ¨: {str(e)}"
        }

# ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ í‘œì‹œ ë¶€ë¶„ ìˆ˜ì •
def display_performance_analysis(analysis: dict):
    """ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ HTMLë¡œ í‘œì‹œ"""
    if not analysis:
        return ""
        
    suggestions_html = "<br>".join(f"- {s}" for s in analysis['suggestions']) if analysis['suggestions'] else "- í˜„ì¬ ì œì•ˆì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."
    
    return f"""
    <div class="prompt-feedback">
        <h4>ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„</h4>
        <p>í˜„ì¬ í‰ê·  ì ìˆ˜: {analysis['current_score']:.1f}</p>
        <p>ì´ì „ ëŒ€ë¹„: {analysis['improvement']:+.1f}</p>
        <p>ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {analysis['top_model'].upper()}</p>
        
        <div class="improvement-tip">
            ğŸ’¡ ê°œì„  í¬ì¸íŠ¸:<br>
            {suggestions_html}
        </div>
    </div>
    """

def analyze_prompt_performance(history: List[dict]) -> dict:
    """í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¶„ì„"""
    if not history:
        return None
    
    try:
        latest = history[-1]
        prev = history[-2] if len(history) > 1 else None
        
        # ì„±ê³µí•œ ëª¨ë¸ì˜ ì ìˆ˜ì™€ í‰ê°€ ì´ìœ  ìˆ˜ì§‘
        valid_scores = []
        evaluation_reasons = []
        for model, eval_data in latest['evaluations'].items():
            if isinstance(eval_data, dict) and eval_data.get('score', 0) > 0:
                valid_scores.append(eval_data['score'])
                if 'reason' in eval_data:
                    evaluation_reasons.append(eval_data['reason'])
        
        if not valid_scores:
            return {
                "current_score": 0,
                "improvement": 0,
                "top_model": "ì—†ìŒ",
                "suggestions": ["í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."]
            }
        
        current_avg = sum(valid_scores) / len(valid_scores)
        
        # ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
        improvement = 0
        if prev:
            prev_valid_scores = [
                e.get('score', 0) 
                for e in prev['evaluations'].values() 
                if isinstance(e, dict) and e.get('score', 0) > 0
            ]
            if prev_valid_scores:
                prev_avg = sum(prev_valid_scores) / len(prev_valid_scores)
                improvement = current_avg - prev_avg
        
        # ìµœê³ /ìµœì € ì„±ëŠ¥ ëª¨ë¸ ë° ì ìˆ˜ ì°¾ê¸°
        valid_models = {
            model: data.get('score', 0)
            for model, data in latest['evaluations'].items()
            if isinstance(data, dict) and data.get('score', 0) > 0
        }
        
        top_model = max(valid_models.items(), key=lambda x: x[1])[0] if valid_models else "ì—†ìŒ"
        
        # êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = []
        
        # ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
        if current_avg < 60:
            suggestions.extend([
                "í”„ë¡¬í”„íŠ¸ì— íƒ€ê²Ÿ ì„¸ëŒ€ì˜ íŠ¹ì„±ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•´ë³´ì„¸ìš”",
                "ì§€ì—­ì˜ ë…íŠ¹í•œ íŠ¹ì§•ì„ 1-2ê°œ ë” ê°•ì¡°í•´ë³´ì„¸ìš”",
                "ê°ì„±ì  í‘œí˜„ê³¼ êµ¬ì²´ì  ì •ë³´ì˜ ê· í˜•ì„ ì¡°ì •í•´ë³´ì„¸ìš”"
            ])
        elif current_avg < 80:
            suggestions.extend([
                "ì¹´í”¼ì˜ í†¤ì•¤ë§¤ë„ˆë¥¼ íƒ€ê²Ÿ ì„¸ëŒ€ì— ë§ê²Œ ë” ì¡°ì •í•´ë³´ì„¸ìš”",
                "ì§€ì—­ íŠ¹ì„±ì„ ë” ì°½ì˜ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì„¸ìš”"
            ])
            
        # í‰ê°€ ì´ìœ  ê¸°ë°˜ ì œì•ˆ
        low_score_aspects = []
        for reason in evaluation_reasons:
            if "íƒ€ê²Ÿ" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("íƒ€ê²Ÿ ì í•©ì„±")
            if "ì°½ì˜" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ì°½ì˜ì„±")
            if "ì§€ì—­" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ì§€ì—­ íŠ¹ì„±")
            if "ì „ë‹¬" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ë©”ì‹œì§€ ì „ë‹¬ë ¥")
        
        if low_score_aspects:
            if "íƒ€ê²Ÿ ì í•©ì„±" in low_score_aspects:
                suggestions.append(f"ì„ íƒí•œ ì„¸ëŒ€({latest['settings']['generation']})ì˜ ê´€ì‹¬ì‚¬ì™€ ì–¸ì–´ ìŠ¤íƒ€ì¼ì„ ë” ë°˜ì˜í•´ë³´ì„¸ìš”")
            if "ì°½ì˜ì„±" in low_score_aspects:
                suggestions.append("ì§„ë¶€í•œ í‘œí˜„ì„ í”¼í•˜ê³  ë” ì‹ ì„ í•œ ë¹„ìœ ë‚˜ í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”")
            if "ì§€ì—­ íŠ¹ì„±" in low_score_aspects:
                suggestions.append(f"{latest['settings']['region']}ë§Œì˜ ë…íŠ¹í•œ ë§¤ë ¥ì„ ë” ë¶€ê°í•´ë³´ì„¸ìš”")
            if "ë©”ì‹œì§€ ì „ë‹¬ë ¥" in low_score_aspects:
                suggestions.append("í•µì‹¬ ë©”ì‹œì§€ë¥¼ ë” ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆê²Œ ì „ë‹¬í•´ë³´ì„¸ìš”")
        
        # ê°œì„ ë„ ê¸°ë°˜ ì œì•ˆ
        if improvement < 0:
            suggestions.append("ì´ì „ í”„ë¡¬í”„íŠ¸ì—ì„œ ì˜ ì‘ë™í–ˆë˜ ìš”ì†Œë“¤ì„ ë‹¤ì‹œ í™œìš©í•´ë³´ì„¸ìš”")
        
        # ì¤‘ë³µ ì œê±°
        suggestions = list(set(suggestions))
        
        return {
            "current_score": current_avg,
            "improvement": improvement,
            "top_model": top_model,
            "suggestions": suggestions[:3]  # ê°€ì¥ ì¤‘ìš”í•œ 3ê°œë§Œ í‘œì‹œ
        }
        
    except Exception as e:
        return {
            "current_score": 0,
            "improvement": 0,
            "top_model": "ë¶„ì„ ì‹¤íŒ¨",
            "suggestions": ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]
        }


def extract_copy_and_description(result_text):
    """ì¹´í”¼ì™€ ì„¤ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if isinstance(result_text, str):
        if "**ì¹´í”¼**:" in result_text and "**ì„¤ëª…**:" in result_text:
            match = re.search(r"\*\*ì¹´í”¼\*\*:\s*(.*?)\s*\*\*ì„¤ëª…\*\*:\s*(.*)", result_text, re.DOTALL)
            if match:
                copy_text = match.group(1).strip()
                description_text = match.group(2).strip()
                return copy_text, description_text
        elif "**ì¹´í”¼**:" in result_text:
            match = re.search(r"\*\*ì¹´í”¼\*\*:\s*(.*)", result_text, re.DOTALL)
            if match:
                copy_text = match.group(1).strip()
                return copy_text, "ì„¤ëª… ì—†ìŒ"
        elif "**ì„¤ëª…**:" in result_text:
            match = re.search(r"\*\*ì„¤ëª…\*\*:\s*(.*)", result_text, re.DOTALL)
            if match:
                description_text = match.group(1).strip()
                return "ì¹´í”¼ ì—†ìŒ", description_text
    return "ì¹´í”¼ ì—†ìŒ", "ì„¤ëª… ì—†ìŒ"
