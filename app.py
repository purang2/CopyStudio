
import streamlit as st
#import openai
from openai import OpenAI
import google.generativeai as genai
from anthropic import Anthropic
from datetime import datetime
import pandas as pd
import json
import pathlib
from typing import Dict, List, Optional
from dataclasses import dataclass

import plotly.express as px 
import plotly.graph_objects as go
from typing import Dict, List, Optional, Union

from google.generativeai.types import HarmCategory, HarmBlockThreshold
from google.api_core.exceptions import ResourceExhausted

import folium
from streamlit_folium import folium_static



# Page config must be the first Streamlit command
st.set_page_config(
    page_title="ê´‘ê³ ì¹´í”¼ ë¬¸êµ¬ ìƒì„± AI", 
    page_icon="ğŸ“’", 
    layout="wide"
)

# ì•± ì œëª©
st.title("ğŸ» ê´‘ê³ ì¹´í”¼ ë¬¸êµ¬ ìƒì„± AI")



# Initialize API keys from Streamlit secrets
#openai.api_key = st.secrets["chatgpt"]
genai.configure(api_key=st.secrets["gemini"])
anthropic = Anthropic(api_key=st.secrets["claude"])
client = OpenAI(api_key=st.secrets["chatgpt"])  # API í‚¤ ì…ë ¥



#ì±—-ì œ-í´ ìˆœì„œ ì˜¤ì™€ì—´
#'gemini-1.5-pro-exp-0827'
#'gemini-1.5-pro-002'
model_zoo = ['gpt-4o',
             'gemini-1.5-flash-002',
             'claude-3-5-haiku-20241022']

# Gemini model configuration
gemini_model = genai.GenerativeModel(model_zoo[1])

# Custom CSS ë¶€ë¶„ì„ ìˆ˜ì •
st.markdown("""
<style>
    @import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css');

    [data-testid="stAppViewContainer"] {
        font-family: 'Pretendard', sans-serif;
    }

    /* ë‹¤í¬ëª¨ë“œ ëŒ€ì‘ì„ ìœ„í•œ CSS ë³€ìˆ˜ í™œìš© */
    :root {
        --text-color: #1e293b;
        --bg-color: #ffffff;
        --card-bg: #ffffff;
        --border-color: #e2e8f0;
        --hover-border: #3b82f6;
        --prompt-bg: #f1f5f9;
    }

    /* ë‹¤í¬ëª¨ë“œì¼ ë•Œì˜ ìƒ‰ìƒ */
    [data-theme="dark"] {
        --text-color: #e2e8f0;
        --bg-color: #1e1e1e;
        --card-bg: #2d2d2d;
        --border-color: #4a4a4a;
        --hover-border: #60a5fa;
        --prompt-bg: #2d2d2d;
    }

    .prompt-editor {
        border: 2px solid var(--border-color);
        border-radius: 10px;
        padding: 1rem;
        background-color: var(--card-bg);
        color: var(--text-color);
    }

    .prompt-editor:hover {
        border-color: var(--hover-border);
        box-shadow: 0 0 0 1px var(--hover-border);
    }

    .prompt-tip {
        background-color: var(--prompt-bg);
        border-left: 4px solid var(--hover-border);
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 8px 8px 0;
        color: var(--text-color);
    }

    .result-card {
        background-color: var(--card-bg);
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        transition: all 0.2s ease;
        color: var(--text-color);
        border: 1px solid var(--border-color);
    }
    
    .result-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .model-tag {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 9999px;
        font-size: 0.875rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: white;
    }

    .score-badge {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.5rem 1rem;
        border-radius: 9999px;
        font-weight: 600;
        background-color: var(--prompt-bg);
        color: var(--text-color);
        cursor: pointer;
    }
    
    .score-badge:hover {
        background-color: var(--border-color);
    }

    .history-item {
        border-left: 4px solid var(--hover-border);
        padding: 1rem;
        margin: 1rem 0;
        background-color: var(--card-bg);
        border-radius: 0 8px 8px 0;
        color: var(--text-color);
    }

    .prompt-feedback {
        background-color: var(--prompt-bg);
        border-radius: 8px;
        padding: 1rem;
        margin-top: 1rem;
        color: var(--text-color);
    }

    .improvement-tip {
        color: var(--hover-border);
        font-weight: 500;
    }

    /* Expander ìŠ¤íƒ€ì¼ë§ */
    .streamlit-expanderHeader {
        background-color: var(--prompt-bg);
        border: 1px solid var(--border-color);
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        color: var(--text-color);
    }
    
    .streamlit-expanderContent {
        border: 1px solid var(--border-color);
        border-top: none;
        border-radius: 0 0 8px 8px;
        padding: 1rem;
        background-color: var(--card-bg);
        color: var(--text-color);
    }
    
    /* ë¬¸ì„œ ì—ë””í„° ìŠ¤íƒ€ì¼ë§ */
    .stTextArea textarea {
        font-family: 'Pretendard', sans-serif;
        font-size: 0.9rem;
        line-height: 1.5;
        color: var(--text-color);
        background-color: var(--card-bg);
    }
    
    /* í”„ë¡¬í”„íŠ¸ ì„¹ì…˜ êµ¬ë¶„ */
    .prompt-section {
        margin: 1rem 0;
        padding: 1rem;
        background-color: var(--card-bg);
        border-radius: 8px;
        border: 1px solid var(--border-color);
        color: var(--text-color);
    }

    /* Plotly ì°¨íŠ¸ ë‹¤í¬ëª¨ë“œ ëŒ€ì‘ */
    .js-plotly-plot .plotly .modebar {
        background-color: var(--card-bg) !important;
    }

    .js-plotly-plot .plotly .modebar-btn path {
        fill: var(--text-color) !important;
    }
</style>
""", unsafe_allow_html=True)




# MBTI ê·¸ë£¹ ìƒìˆ˜ ì •ì˜
MBTI_GROUPS = {
    "ë¶„ì„ê°€í˜•": ["INTJ", "INTP", "ENTJ", "ENTP"],
    "ì™¸êµê´€í˜•": ["INFJ", "INFP", "ENFJ", "ENFP"],
    "ê´€ë¦¬ìí˜•": ["ISTJ", "ISFJ", "ESTJ", "ESFJ"],
    "íƒí—˜ê°€í˜•": ["ISTP", "ISFP", "ESTP", "ESFP"]
}

# Constants
MBTI_TYPES = [
    "INTJ", "INTP", "ENTJ", "ENTP",
    "INFJ", "INFP", "ENFJ", "ENFP",
    "ISTJ", "ISFJ", "ESTJ", "ESFJ",
    "ISTP", "ISFP", "ESTP", "ESFP"
]

MODEL_COLORS = {
    "gpt": "#10a37f",  # OpenAI ê·¸ë¦°
    "gemini": "#4285f4",  # Google ë¸”ë£¨
    "claude": "#8e44ad"  # Claude í¼í”Œ
}


# ê³„ì ˆ ìƒìˆ˜ ì¶”ê°€
SEASONS = {
    "ë´„": "spring",
    "ì—¬ë¦„": "summer",
    "ê°€ì„": "autumn",
    "ê²¨ìš¸": "winter"
}



@dataclass
class ScoringConfig:
    """í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    prompt: str
    criteria: List[str]
    min_score: int = 0
    max_score: int = 100
    
    def to_dict(self) -> dict:
        return {
            "prompt": self.prompt,
            "criteria": self.criteria,
            "min_score": self.min_score,
            "max_score": self.max_score
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'ScoringConfig':
        return cls(**data)


def load_docs() -> Dict[str, Dict[str, str]]:
    docs = {
        "region": {},
        "generation": {},
        "mbti": {}
    }
    
    docs_path = pathlib.Path("docs")
    
    # Load region docs
    region_path = docs_path / "regions"
    if region_path.exists():
        for file in region_path.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                docs["region"][file.stem] = f.read()
    
    # Load generation docs
    generation_path = docs_path / "generations"
    if generation_path.exists():
        for file in generation_path.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                docs["generation"][file.stem] = f.read()
    
    # Load MBTI docs
    mbti_path = docs_path / "mbti"
    if mbti_path.exists():
        for mbti in MBTI_TYPES:
            mbti_file = mbti_path / f"{mbti}.txt"
            if mbti_file.exists():
                with open(mbti_file, "r", encoding="utf-8") as f:
                    docs["mbti"][mbti] = f.read()
    
    return docs


DOCS = load_docs()


def create_adaptive_prompt(
    city_doc: str, 
    target_generation: str, 
    mbti: str = None,
    include_mbti: bool = False
) -> str:
    """ë¬¸ì„œ ê¸°ë°˜ ìœ ì—°í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    base_prompt = f"""
ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µë˜ëŠ” ë„ì‹œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ë§¤ë ¥ì ì¸ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì¹´í”¼ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

[ë„ì‹œ ì •ë³´]
{city_doc}

[ì¹´í”¼ ì‘ì„± ê°€ì´ë“œë¼ì¸]
1. ìœ„ ì •ë³´ëŠ” ì˜ê°ì„ ì–»ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.
2. ë„ì‹œì˜ í•µì‹¬ ë§¤ë ¥ì„ í¬ì°©í•´ ì‹ ì„ í•œ ê´€ì ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.
3. íƒ€ê²Ÿì¸µì— ë§ëŠ” í†¤ì•¤ë§¤ë„ˆë¥¼ ì‚¬ìš©í•˜ë˜, ì •ë³´ì˜ ë‚˜ì—´ì€ í”¼í•´ì£¼ì„¸ìš”.
4. ê°ì„±ì  ê³µê°ê³¼ êµ¬ì²´ì  íŠ¹ì§•ì´ ì¡°í™”ë¥¼ ì´ë£¨ë„ë¡ í•´ì£¼ì„¸ìš”.

[íƒ€ê²Ÿ ì •ë³´]
ì„¸ëŒ€: {target_generation}"""

    if include_mbti and mbti and mbti in MBTI_TYPES:
        try:
            mbti_content = DOCS["mbti"].get(mbti)
            if mbti_content:
                mbti_prompt = f"""

[MBTI íŠ¹ì„± - {mbti}]
{mbti_content}

íŠ¹ë³„ ê³ ë ¤ì‚¬í•­:
- ìœ„ {mbti} ì„±í–¥ì˜ ì—¬í–‰ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•´ ì¹´í”¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”
- í•´ë‹¹ MBTIì˜ í•µì‹¬ ê°€ì¹˜ê´€ê³¼ ì„ í˜¸ ìŠ¤íƒ€ì¼ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”"""
                base_prompt += mbti_prompt
            else:
                print(f"{mbti}.txt íŒŒì¼ì˜ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"MBTI í”„ë¡¬í”„íŠ¸ ìƒì„± ì—ëŸ¬: {str(e)}")

    base_prompt += """

[ì œì•½ì‚¬í•­]
- í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ì´ëª¨ì§€ 1-2ê°œ í¬í•¨
- ë„ì‹œë§Œì˜ ë…íŠ¹í•œ íŠ¹ì§• í•˜ë‚˜ ì´ìƒ í¬í•¨
- í´ë¦¬ì…°ë‚˜ ì§„ë¶€í•œ í‘œí˜„ ì§€ì–‘
"""
    return base_prompt

# íŒŒì¼ ë¡œë”© í•¨ìˆ˜ì— ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
def load_docs() -> Dict[str, Dict[str, str]]:
    docs = {
        "region": {},
        "generation": {},
        "mbti": {}
    }
    
    try:
        docs_path = pathlib.Path("docs")
        
        # Load region docs
        region_path = docs_path / "regions"
        if region_path.exists():
            for file in region_path.glob("*.txt"):
                with open(file, "r", encoding="utf-8") as f:
                    docs["region"][file.stem] = f.read()
        
        # Load generation docs
        generation_path = docs_path / "generations"
        if generation_path.exists():
            for file in generation_path.glob("*.txt"):
                with open(file, "r", encoding="utf-8") as f:
                    docs["generation"][file.stem] = f.read()
        
        # Load individual MBTI files
        mbti_path = docs_path / "mbti"
        if mbti_path.exists():
            for mbti in MBTI_TYPES:
                mbti_file = mbti_path / f"{mbti}.txt"
                try:
                    if mbti_file.exists():
                        with open(mbti_file, "r", encoding="utf-8") as f:
                            docs["mbti"][mbti] = f.read()
                            print(f"ë¡œë“œëœ MBTI íŒŒì¼: {mbti}.txt")
                    else:
                        print(f"MBTI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mbti}.txt")
                except Exception as e:
                    print(f"{mbti} íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            print(f"MBTI ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mbti_path}")
            
    except Exception as e:
        print(f"ë¬¸ì„œ ë¡œë”© ì—ëŸ¬: {str(e)}")
        st.error(f"ë¬¸ì„œ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ë¡œë“œëœ MBTI íŒŒì¼ ëª©ë¡ ì¶œë ¥
    print(f"ë¡œë“œëœ MBTI ëª©ë¡: {list(docs['mbti'].keys())}")
    return docs






class AdCopyEvaluator:
    """ê´‘ê³  ì¹´í”¼ í‰ê°€ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, scoring_config: ScoringConfig):
        self.scoring_config = scoring_config
        self.results_cache = {}
    
    def evaluate(self, copy: str, model_name: str) -> Dict:
        """í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ íŒŒì‹±"""
        try:
            # ìºì‹œëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            cache_key = f"{copy}_{model_name}"
            if cache_key in self.results_cache:
                return self.results_cache[cache_key]
            
            # í‰ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            evaluation_prompt = f"""
{self.scoring_config.prompt}

í‰ê°€ ëŒ€ìƒ ì¹´í”¼: {copy}

í‰ê°€ ê¸°ì¤€:
{chr(10).join(f'- {criterion}' for criterion in self.scoring_config.criteria)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [í‰ê°€ ê·¼ê±°]
ìƒì„¸ì ìˆ˜: [ê° ê¸°ì¤€ë³„ ì ìˆ˜ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„]
"""
            # API calls by model
            if model_name == "gpt":
                #response = openai.ChatCompletion.create(
                response = client.chat.completions.create(
                    model=model_zoo[0],
                    messages=[{"role": "user", "content": evaluation_prompt}],
                    max_tokens=1000 
                )
                result_text = response.choices[0].message.content
            elif model_name == "gemini":
                try:
                    response = gemini_model.generate_content(evaluation_prompt)
                    #return response
                    return response.text
                
                except Exception as e:
                    return f"Gemini í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            else:  # claude
                response = anthropic.messages.create(
                    model=model_zoo[2],
                    max_tokens=1000,
                    messages=[{"role": "user", "content": evaluation_prompt}]
                )
                result_text = response.content[0].text
            
            # Parse results
            parsed_result = self.parse_evaluation_result(result_text)
            
            # Cache results
            self.results_cache[cache_key] = parsed_result
            
            return parsed_result
            
        except Exception as e:
            st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "score": 0,
                "reason": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}",
                "detailed_scores": [0] * len(self.scoring_config.criteria)
            }

    def parse_evaluation_result(self, result_text: str) -> Dict:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        try:
            lines = result_text.split('\n')
            
            # ì ìˆ˜ ì¶”ì¶œ ê°œì„ 
            score_line = next(l for l in lines if 'ì ìˆ˜:' in l)
            # ìˆ«ìì™€ ì†Œìˆ˜ì ë§Œ ì¶”ì¶œí•˜ë„ë¡ ìˆ˜ì •
            score_text = score_line.split('ì ìˆ˜:')[1].strip()
            # ìˆ«ìì™€ ì†Œìˆ˜ì ë§Œ ë‚¨ê¸°ê³  ì œê±°
            score_text = ''.join(c for c in score_text if c.isdigit() or c == '.')
            score = float(score_text) if score_text else 0
            
            # ì´ìœ  ì¶”ì¶œ
            reason_line = next(l for l in lines if 'ì´ìœ :' in l)
            reason = reason_line.split('ì´ìœ :')[1].strip()
            
            # ìƒì„¸ì ìˆ˜ ì¶”ì¶œ ê°œì„ 
            try:
                detailed_line = next(l for l in lines if 'ìƒì„¸ì ìˆ˜:' in l)
                detailed_scores_text = detailed_line.split('ìƒì„¸ì ìˆ˜:')[1].strip()
                detailed_scores = []
                
                for s in detailed_scores_text.split(','):
                    s = s.strip()
                    # ê° ì ìˆ˜ì—ë„ ì†Œìˆ˜ì  ì²˜ë¦¬ ì ìš©
                    score_text = ''.join(c for c in s if c.isdigit() or c == '.')
                    detailed_scores.append(float(score_text) if score_text else 0)
            except:
                detailed_scores = [score] * len(self.scoring_config.criteria)
            
            return {
                "score": score,
                "reason": reason,
                "detailed_scores": detailed_scores[:len(self.scoring_config.criteria)]
            }
        except Exception as e:
            st.error(f"ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "score": 0,
                "reason": f"íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                "detailed_scores": [0] * len(self.scoring_config.criteria)
            }
            
def generate_copy(prompt: str, model_name: str) -> Union[str, Dict]:
    """ê´‘ê³  ì¹´í”¼ ìƒì„±"""
    try:
        if model_name == "gpt":
            response = client.chat.completions.create(
                model=model_zoo[0],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000
            )
            return {
                "success": True,
                "content": response.choices[0].message.content.strip()
            }
            
        elif model_name == "gemini":
            try:
                response = gemini_model.generate_content(prompt)  # ë‹¨ìˆœí™”
                generated_text = response.text.strip()  # ë°”ë¡œ text ì¶”ì¶œ
                
                if generated_text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                    return {
                        "success": True,
                        "content": generated_text
                    }
                else:
                    return {
                        "success": False,
                        "content": "Geminiê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    }
                    
            except Exception as e:
                print(f"Gemini ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš©
                return {
                    "success": False,
                    "content": f"Gemini API ì˜¤ë¥˜: {str(e)}"
                }
            
        else:  # claude
            try:
                response = anthropic.messages.create(
                    model=model_zoo[2],
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=1000,
                    temperature=0.7
                )
                return {
                    "success": True,
                    "content": response.content[0].text.strip()
                }
            except Exception as e:
                return {
                    "success": False,
                    "content": f"Claude API ì˜¤ë¥˜: {str(e)}"
                }
                
    except Exception as e:
        return {
            "success": False,
            "content": f"ìƒì„± ì‹¤íŒ¨: {str(e)}"
        }

# ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ í‘œì‹œ ë¶€ë¶„ ìˆ˜ì •
def display_performance_analysis(analysis: dict):
    """ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ HTMLë¡œ í‘œì‹œ"""
    if not analysis:
        return ""
        
    suggestions_html = "<br>".join(f"- {s}" for s in analysis['suggestions']) if analysis['suggestions'] else "- í˜„ì¬ ì œì•ˆì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."
    
    return f"""
    <div class="prompt-feedback">
        <h4>ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„</h4>
        <p>í˜„ì¬ í‰ê·  ì ìˆ˜: {analysis['current_score']:.1f}</p>
        <p>ì´ì „ ëŒ€ë¹„: {analysis['improvement']:+.1f}</p>
        <p>ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {analysis['top_model'].upper()}</p>
        
        <div class="improvement-tip">
            ğŸ’¡ ê°œì„  í¬ì¸íŠ¸:<br>
            {suggestions_html}
        </div>
    </div>
    """


def visualize_evaluation_results(eval_data: Dict):
    """ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜"""
    if not eval_data:
        return None

    # í‰ê°€ ì ìˆ˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê°€ì ¸ì˜¤ê¸°
    scores = eval_data.get('detailed_scores', [0] * len(st.session_state.scoring_config.criteria))
    criteria = st.session_state.scoring_config.criteria[:len(scores)]

    # ìµœì†Œ 3ê°œ ì´ìƒì˜ ì¶•ì´ í•„ìš”í•˜ë„ë¡ ë³´ì •
    if len(criteria) < 3:
        criteria.extend(['ì¶”ê°€ ê¸°ì¤€'] * (3 - len(criteria)))
        scores.extend([0] * (3 - len(scores)))  # ê´„í˜¸ ì¶”ê°€

    # ì°¨íŠ¸ ìƒì„±
    fig = go.Figure(data=go.Scatterpolar(
        r=scores,
        theta=criteria,
        fill='toself',
        name='í‰ê°€ ì ìˆ˜'
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )
        ),
        showlegend=False,
        title="í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜"
    )

    # ì°¨íŠ¸ë§Œ í‘œì‹œ
    st.plotly_chart(fig, use_container_width=True)

def analyze_prompt_performance(history: List[dict]) -> dict:
    """í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¶„ì„"""
    if not history:
        return None
    
    try:
        latest = history[-1]
        prev = history[-2] if len(history) > 1 else None
        
        # ì„±ê³µí•œ ëª¨ë¸ì˜ ì ìˆ˜ì™€ í‰ê°€ ì´ìœ  ìˆ˜ì§‘
        valid_scores = []
        evaluation_reasons = []
        for model, eval_data in latest['evaluations'].items():
            if isinstance(eval_data, dict) and eval_data.get('score', 0) > 0:
                valid_scores.append(eval_data['score'])
                if 'reason' in eval_data:
                    evaluation_reasons.append(eval_data['reason'])
        
        if not valid_scores:
            return {
                "current_score": 0,
                "improvement": 0,
                "top_model": "ì—†ìŒ",
                "suggestions": ["í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."]
            }
        
        current_avg = sum(valid_scores) / len(valid_scores)
        
        # ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
        improvement = 0
        if prev:
            prev_valid_scores = [
                e.get('score', 0) 
                for e in prev['evaluations'].values() 
                if isinstance(e, dict) and e.get('score', 0) > 0
            ]
            if prev_valid_scores:
                prev_avg = sum(prev_valid_scores) / len(prev_valid_scores)
                improvement = current_avg - prev_avg
        
        # ìµœê³ /ìµœì € ì„±ëŠ¥ ëª¨ë¸ ë° ì ìˆ˜ ì°¾ê¸°
        valid_models = {
            model: data.get('score', 0)
            for model, data in latest['evaluations'].items()
            if isinstance(data, dict) and data.get('score', 0) > 0
        }
        
        top_model = max(valid_models.items(), key=lambda x: x[1])[0] if valid_models else "ì—†ìŒ"
        
        # êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = []
        
        # ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
        if current_avg < 60:
            suggestions.extend([
                "í”„ë¡¬í”„íŠ¸ì— íƒ€ê²Ÿ ì„¸ëŒ€ì˜ íŠ¹ì„±ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•´ë³´ì„¸ìš”",
                "ì§€ì—­ì˜ ë…íŠ¹í•œ íŠ¹ì§•ì„ 1-2ê°œ ë” ê°•ì¡°í•´ë³´ì„¸ìš”",
                "ê°ì„±ì  í‘œí˜„ê³¼ êµ¬ì²´ì  ì •ë³´ì˜ ê· í˜•ì„ ì¡°ì •í•´ë³´ì„¸ìš”"
            ])
        elif current_avg < 80:
            suggestions.extend([
                "ì¹´í”¼ì˜ í†¤ì•¤ë§¤ë„ˆë¥¼ íƒ€ê²Ÿ ì„¸ëŒ€ì— ë§ê²Œ ë” ì¡°ì •í•´ë³´ì„¸ìš”",
                "ì§€ì—­ íŠ¹ì„±ì„ ë” ì°½ì˜ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì„¸ìš”"
            ])
            
        # í‰ê°€ ì´ìœ  ê¸°ë°˜ ì œì•ˆ
        low_score_aspects = []
        for reason in evaluation_reasons:
            if "íƒ€ê²Ÿ" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("íƒ€ê²Ÿ ì í•©ì„±")
            if "ì°½ì˜" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ì°½ì˜ì„±")
            if "ì§€ì—­" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ì§€ì—­ íŠ¹ì„±")
            if "ì „ë‹¬" in reason.lower() and "ë¶€ì¡±" in reason:
                low_score_aspects.append("ë©”ì‹œì§€ ì „ë‹¬ë ¥")
        
        if low_score_aspects:
            if "íƒ€ê²Ÿ ì í•©ì„±" in low_score_aspects:
                suggestions.append(f"ì„ íƒí•œ ì„¸ëŒ€({latest['settings']['generation']})ì˜ ê´€ì‹¬ì‚¬ì™€ ì–¸ì–´ ìŠ¤íƒ€ì¼ì„ ë” ë°˜ì˜í•´ë³´ì„¸ìš”")
            if "ì°½ì˜ì„±" in low_score_aspects:
                suggestions.append("ì§„ë¶€í•œ í‘œí˜„ì„ í”¼í•˜ê³  ë” ì‹ ì„ í•œ ë¹„ìœ ë‚˜ í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”")
            if "ì§€ì—­ íŠ¹ì„±" in low_score_aspects:
                suggestions.append(f"{latest['settings']['region']}ë§Œì˜ ë…íŠ¹í•œ ë§¤ë ¥ì„ ë” ë¶€ê°í•´ë³´ì„¸ìš”")
            if "ë©”ì‹œì§€ ì „ë‹¬ë ¥" in low_score_aspects:
                suggestions.append("í•µì‹¬ ë©”ì‹œì§€ë¥¼ ë” ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆê²Œ ì „ë‹¬í•´ë³´ì„¸ìš”")
        
        # ê°œì„ ë„ ê¸°ë°˜ ì œì•ˆ
        if improvement < 0:
            suggestions.append("ì´ì „ í”„ë¡¬í”„íŠ¸ì—ì„œ ì˜ ì‘ë™í–ˆë˜ ìš”ì†Œë“¤ì„ ë‹¤ì‹œ í™œìš©í•´ë³´ì„¸ìš”")
        
        # ì¤‘ë³µ ì œê±°
        suggestions = list(set(suggestions))
        
        return {
            "current_score": current_avg,
            "improvement": improvement,
            "top_model": top_model,
            "suggestions": suggestions[:3]  # ê°€ì¥ ì¤‘ìš”í•œ 3ê°œë§Œ í‘œì‹œ
        }
        
    except Exception as e:
        return {
            "current_score": 0,
            "improvement": 0,
            "top_model": "ë¶„ì„ ì‹¤íŒ¨",
            "suggestions": ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]
        }

def visualize_evaluation_results(results: Dict):
    """ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜"""
    if not results or 'detailed_scores' not in results:
        return None
        
    # í˜„ì¬ ì„¤ì •ëœ í‰ê°€ ê¸°ì¤€ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
    scores = results['detailed_scores'][:len(st.session_state.scoring_config.criteria)]
    criteria = st.session_state.scoring_config.criteria[:len(scores)]
    
    # ìµœì†Œ 3ê°œ ì´ìƒì˜ ì¶•ì´ í•„ìš”í•˜ë„ë¡ ë³´ì •
    if len(criteria) < 3:
        criteria.extend(['ì¶”ê°€ ê¸°ì¤€'] * (3 - len(criteria)))
        scores.extend([0] * (3 - len(scores)))
    
    fig = go.Figure(data=go.Scatterpolar(
        r=scores,
        theta=criteria,
        fill='toself',
        name='í‰ê°€ ì ìˆ˜'
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )
        ),
        showlegend=False,
        title="í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜"
    )
    return fig


# Load documents
DOCS = load_docs()

# Initialize session state
if 'history' not in st.session_state:
    st.session_state.history = []
if 'show_tutorial' not in st.session_state:
    st.session_state.show_tutorial = True

# Initialize scoring config
DEFAULT_SCORING_CONFIG = ScoringConfig(
    prompt="""
ì£¼ì–´ì§„ ê´‘ê³  ì¹´í”¼ë¥¼ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
ê° ê¸°ì¤€ë³„ë¡œ 0-100ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³ , 
ìµœì¢… ì ìˆ˜ëŠ” ê° ê¸°ì¤€ì˜ í‰ê· ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """,
    criteria=[
        "íƒ€ê²Ÿ ì„¸ëŒ€ ì í•©ì„±",
        "ë©”ì‹œì§€ ì „ë‹¬ë ¥",
        "ì°½ì˜ì„±",
        "ì§€ì—­ íŠ¹ì„± ë°˜ì˜ë„"
    ]
)



if 'scoring_config' not in st.session_state:
    st.session_state.scoring_config = DEFAULT_SCORING_CONFIG
if 'evaluator' not in st.session_state:
    st.session_state.evaluator = AdCopyEvaluator(st.session_state.scoring_config)



# Tutorial
if st.session_state.show_tutorial:
    with st.sidebar:
        st.info("""
        ğŸ‘‹ ì²˜ìŒ ì˜¤ì…¨ë‚˜ìš”?
        
        1ï¸âƒ£ ì§€ì—­ê³¼ ì„¸ëŒ€ë¥¼ ì„ íƒí•˜ì„¸ìš”
        2ï¸âƒ£ ê³„ì ˆê³¼ MBTIë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
        3ï¸âƒ£ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ê²€í† /ìˆ˜ì •í•˜ì„¸ìš”
        4ï¸âƒ£ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì„¸ìš”
        
        ğŸ¯ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ë©° ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!
        """)
        if st.button("ì•Œê² ìŠµë‹ˆë‹¤!", use_container_width=True):
            st.session_state.show_tutorial = False

# Sidebar
with st.sidebar:
    # í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì • ë¶€ë¶„ ì¶”ê°€
    st.header("âš™ï¸ í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì •")
    
    with st.expander("í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
        new_prompt = st.text_area(
            "í‰ê°€ í”„ë¡¬í”„íŠ¸",
            value=st.session_state.scoring_config.prompt
        )
        new_criteria = st.text_area(
            "í‰ê°€ ê¸°ì¤€ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
            value="\n".join(st.session_state.scoring_config.criteria)
        )
        
        if st.button("í‰ê°€ ì„¤ì • ì—…ë°ì´íŠ¸"):
            new_config = ScoringConfig(
                prompt=new_prompt,
                criteria=[c.strip() for c in new_criteria.split('\n') if c.strip()]
            )
            st.session_state.scoring_config = new_config
            st.session_state.evaluator = AdCopyEvaluator(new_config)
            st.success("í‰ê°€ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.title("ğŸ¯ íƒ€ê²Ÿ ì„¤ì •")
    
    selected_region = st.selectbox(
        "ì§€ì—­ ì„ íƒ",
        options=[""] + list(DOCS["region"].keys()),
        format_func=lambda x: "ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )
    
    selected_generation = st.selectbox(
        "ì„¸ëŒ€ ì„ íƒ",
        options=[""] + list(DOCS["generation"].keys()),
        format_func=lambda x: "ì„¸ëŒ€ë¥¼ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )

    # ê³„ì ˆ ì„ íƒ ì¶”ê°€
    selected_season = st.selectbox(
        "ê³„ì ˆ ì„ íƒ (ì„ íƒì‚¬í•­)",
        options=[""] + list(SEASONS.keys()),
        format_func=lambda x: "ê³„ì ˆì„ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )
    
    include_mbti = st.checkbox("MBTI íŠ¹ì„± í¬í•¨í•˜ê¸°")
    selected_mbti = None
    if include_mbti:
        selected_mbti = st.selectbox(
            "MBTI ì„ íƒ",
            options=MBTI_TYPES,
            help="ì„ íƒí•œ MBTI ì„±í–¥ì— ë§ëŠ” ì¹´í”¼ê°€ ìƒì„±ë©ë‹ˆë‹¤"
        )
# Main content
col1, col2 = st.columns([3, 2])

with col1:
    st.subheader("ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì‘ì„±")
    
    # í”„ë¡¬í”„íŠ¸ ì—ë””í„° ì˜ì—­
    st.markdown("""
    <div class="prompt-tip">
        ğŸ’¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”.
        ë¬¸ì„œ ë‚´ìš©ì€ ì ‘ì–´ë‘ê³  í•„ìš”í•  ë•Œ í¼ì³ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
    </div>
    """, unsafe_allow_html=True)
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
    base_structure = """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µë˜ëŠ” ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ë§¤ë ¥ì ì¸ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
íƒ€ê²Ÿ ê³ ê°ì„ ë§¤í˜¹í•˜ëŠ” ë§ˆì¼€íŒ…ì„ í•˜ê¸° ìœ„í•´ ê³ ê°ì´ ì†í•œ ì„¸ëŒ€ì™€ MBTIì˜ íŠ¹ì„±ì„ ì œê³µí•˜ë‹ˆ ì˜ ì°¸ê³ í•´ì£¼ì„¸ìš”.  
ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì¹´í”¼ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤."""

    st.markdown("#### ê¸°ë³¸ ì„¤ì •")
    st.markdown(base_structure)

    # ë¬¸ì„œ ë‚´ìš©ì„ expanderë¡œ í‘œì‹œ
    with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë‚´ìš© ë³´ê¸°/ìˆ˜ì •", expanded=False):
        docs_content = f"""
### ì§€ì—­ ì •ë³´
{DOCS["region"].get(selected_region, "ì§€ì—­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")}

### ì„¸ëŒ€ íŠ¹ì„±
{DOCS["generation"].get(selected_generation, "ì„¸ëŒ€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")}
"""
        if include_mbti and selected_mbti:
            docs_content += f"""
### MBTI íŠ¹ì„±
{DOCS["mbti"].get(selected_mbti, f"{selected_mbti} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")}
"""
        if selected_season:
            docs_content += f"""
### ê³„ì ˆ íŠ¹ì„±
{selected_season}ì˜ íŠ¹ì§•ì„ ë°˜ì˜í•©ë‹ˆë‹¤."""

        edited_docs = st.text_area(
            "ë¬¸ì„œ ë‚´ìš© ìˆ˜ì •",
            value=docs_content,
            height=300,
            key="docs_editor"
        )
    
    st.markdown("#### ìš”êµ¬ì‚¬í•­")
    requirements = """
1. ìœ„ ì •ë³´ëŠ” ì˜ê°ì„ ì–»ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.
2. ë„ì‹œì˜ í•µì‹¬ ë§¤ë ¥ì„ í¬ì°©í•´ ì‹ ì„ í•œ ê´€ì ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.
3. íƒ€ê²Ÿì¸µì— ë§ëŠ” í†¤ì•¤ë§¤ë„ˆë¥¼ ì‚¬ìš©í•˜ë˜, ì •ë³´ì˜ ë‚˜ì—´ì€ í”¼í•´ì£¼ì„¸ìš”.
4. ê°ì„±ì  ê³µê°ê³¼ êµ¬ì²´ì  íŠ¹ì§•ì´ ì¡°í™”ë¥¼ ì´ë£¨ë„ë¡ í•´ì£¼ì„¸ìš”.
5. í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ì´ëª¨ì§€ 1-2ê°œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""
    st.markdown(requirements)

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ë° ìˆ˜ì •
    st.markdown("#### ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸")
    edited_prompt = st.text_area(
        "í”„ë¡¬í”„íŠ¸ ì§ì ‘ ìˆ˜ì •",
        value=base_structure + "\n\n" + edited_docs + "\n\nìš”êµ¬ì‚¬í•­:\n" + requirements,
        height=200,
        key="final_prompt"
    )

    # ìƒì„± ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œì˜ ë¡œì§ ìˆ˜ì •
    if st.button("ğŸ¨ ê´‘ê³  ì¹´í”¼ ìƒì„±", use_container_width=True):
        if not selected_region or not selected_generation:
            st.error("ì§€ì—­ê³¼ ì„¸ëŒ€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner("AI ëª¨ë¸ì´ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                results = {}
                evaluations = {}
                
                for model in ["gpt", "gemini", "claude"]:
                    result = generate_copy(edited_prompt, model)
                    
                    # resultê°€ ë¬¸ìì—´ì¸ì§€ ë¨¼ì € í™•ì¸í•˜ê³  ë¬¸ìì—´ì¼ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬
                    if isinstance(result, dict) and result.get("success"):
                        # resultê°€ dictì¼ ê²½ìš° ì •ìƒ ì²˜ë¦¬
                        results[model] = result["content"]
                        eval_result = st.session_state.evaluator.evaluate(result["content"], "gpt")  # í‰ê°€ ì‹œ gptë¡œ ê³ ì •
                        evaluations[model] = eval_result
                    elif isinstance(result, str):
                        # gemini/claudeê°€ ë¬¸ìì—´ë¡œ ìƒì„±í•œ ê²°ê³¼ë¥¼ gptë¡œ í‰ê°€
                        results[model] = result
                        eval_result = st.session_state.evaluator.evaluate(result, "gpt")  # í‰ê°€ ì‹œ gptë¡œ ê³ ì •
                        evaluations[model] = eval_result
                    else:
                        # ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê°’ ì„¤ì •
                        results[model] = "ê²°ê³¼ ì—†ìŒ"
                        evaluations[model] = {
                            "score": 0,
                            "reason": "í‰ê°€ ì‹¤íŒ¨",
                            "detailed_scores": [0] * len(st.session_state.scoring_config.criteria)
                        }
                
                experiment_data = {
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "prompt": edited_prompt,
                    "results": results,
                    "evaluations": evaluations,
                    "settings": {
                        "region": selected_region,
                        "generation": selected_generation,
                        "season": selected_season if selected_season else None,
                        "mbti": selected_mbti if include_mbti else None
                    }
                }
                st.session_state.history.append(experiment_data)


                
# with col2 ë¶€ë¶„ì˜ ì„±ëŠ¥ ë¶„ì„ í‘œì‹œ ì½”ë“œë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •
with col2:
    st.subheader("ì‹¤í—˜ ê²°ê³¼")
    
    if st.session_state.history:
        latest_experiment = st.session_state.history[-1]
        
        # ì„±ëŠ¥ ë¶„ì„
        analysis = analyze_prompt_performance(st.session_state.history)
        if analysis:
            try:
                # HTML íƒœê·¸ê°€ ë…¸ì¶œë˜ì§€ ì•Šë„ë¡ ì»¨í…Œì´ë„ˆì™€ ë§ˆí¬ë‹¤ìš´ ì‚¬ìš©
                with st.container():
                    st.markdown("### ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")
                    st.write(f"í˜„ì¬ í‰ê·  ì ìˆ˜: {analysis['current_score']:.1f}")
                    st.write(f"ì´ì „ ëŒ€ë¹„: {analysis['improvement']:+.1f}")
                    st.write(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {analysis['top_model'].upper()}")
                    
                    # ê°œì„  í¬ì¸íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
                    if analysis['suggestions']:
                        st.markdown("#### ğŸ’¡ ê°œì„  í¬ì¸íŠ¸:")
                        for suggestion in analysis['suggestions']:
                            st.markdown(f"- {suggestion}")
                    
            except Exception as e:
                st.error(f"ì„±ëŠ¥ ë¶„ì„ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ê²°ê³¼ ì¹´ë“œ í‘œì‹œ
        model_list = ["gpt", "gemini", "claude"]
        for idx, model_name in enumerate(model_list):
            try:
                with st.container():
                    # 'latest_experiment['results']'ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
                    if isinstance(latest_experiment.get('results'), dict):
                        result = latest_experiment['results'].get(model_name, "ê²°ê³¼ ì—†ìŒ")
                    else:
                        result = "ê²°ê³¼ ì—†ìŒ"
                    
                    # 'latest_experiment['evaluations']'ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
                    eval_data = (latest_experiment.get('evaluations', {}).get(model_name) 
                                 if isinstance(latest_experiment.get('evaluations'), dict) 
                                 else {
                                     "score": 0,
                                     "reason": "í‰ê°€ ì‹¤íŒ¨",
                                     "detailed_scores": [0] * len(st.session_state.scoring_config.criteria)
                                 })
        
                    st.markdown(f"""
                    <div class="result-card">
                        <span class="model-tag" style="background-color: {MODEL_COLORS[model_name]}">
                            {model_name.upper()}
                        </span>
                        <div style="margin: 1rem 0;">
                            {result}
                        </div>
                        <div class="score-badge">
                            ì ìˆ˜: {eval_data.get('score', 0)}ì 
                        </div>
                        <div class="prompt-feedback">
                            {eval_data.get('reason', 'í‰ê°€ ì´ìœ  ì—†ìŒ')}
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if 'detailed_scores' in eval_data:
                        try:
                            fig = visualize_evaluation_results(eval_data)
                            st.plotly_chart(fig, use_container_width=True)
                        except Exception as e:
                            st.error(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            except Exception as e:
                st.error(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({model_name}): {str(e)}")

    else:
        st.info("ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•˜ë©´ ì—¬ê¸°ì— ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
