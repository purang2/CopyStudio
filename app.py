import streamlit as st
#import openai
from openai import OpenAI
import google.generativeai as genai
from anthropic import Anthropic
from datetime import datetime
import pandas as pd
import json
import pathlib
from typing import Dict, List, Optional
from dataclasses import dataclass

import plotly.express as px 
import plotly.graph_objects as go


# Page config must be the first Streamlit command
st.set_page_config(
    page_title="CopyStudio Lab - ê´‘ê³  ì¹´í”¼ ì—°êµ¬ì†Œ", 
    page_icon="ğŸ”¬", 
    layout="wide"
)

# Initialize API keys from Streamlit secrets
#openai.api_key = st.secrets["chatgpt"]
genai.configure(api_key=st.secrets["gemini"])
anthropic = Anthropic(api_key=st.secrets["claude"])
client = OpenAI(api_key=st.secrets["chatgpt"])  # API í‚¤ ì…ë ¥



#ì±—-ì œ-í´ ìˆœì„œ ì˜¤ì™€ì—´
model_zoo = ['gpt-4o',
             'gemini-1.5-pro-exp-0827',
             'claude-3-5-haiku-20241022']

# Gemini model configuration
gemini_model = genai.GenerativeModel(model_zoo[1])


# Custom CSS
st.markdown("""
<style>
    @import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css');

    [data-testid="stAppViewContainer"] {
        font-family: 'Pretendard', sans-serif;
        background-color: #f8fafc;
    }

    .prompt-editor {
        border: 2px solid #e2e8f0;
        border-radius: 10px;
        padding: 1rem;
        background-color: white;
    }

    .prompt-editor:hover {
        border-color: #3b82f6;
        box-shadow: 0 0 0 1px #3b82f6;
    }

    .prompt-tip {
        background-color: #f1f5f9;
        border-left: 4px solid #3b82f6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 8px 8px 0;
    }

    .result-card {
        background-color: white;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        transition: all 0.2s ease;
    }
    
    .result-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .model-tag {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 9999px;
        font-size: 0.875rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: white;
    }

    .score-badge {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.5rem 1rem;
        border-radius: 9999px;
        font-weight: 600;
        background-color: #f1f5f9;
        cursor: pointer;
    }
    
    .score-badge:hover {
        background-color: #e2e8f0;
    }

    .history-item {
        border-left: 4px solid #3b82f6;
        padding: 1rem;
        margin: 1rem 0;
        background-color: white;
        border-radius: 0 8px 8px 0;
    }

    .prompt-feedback {
        background-color: #f8fafc;
        border-radius: 8px;
        padding: 1rem;
        margin-top: 1rem;
    }

    .improvement-tip {
        color: #3b82f6;
        font-weight: 500;
    }
</style>
""", unsafe_allow_html=True)




# MBTI ê·¸ë£¹ ìƒìˆ˜ ì •ì˜
MBTI_GROUPS = {
    "ë¶„ì„ê°€í˜•": ["INTJ", "INTP", "ENTJ", "ENTP"],
    "ì™¸êµê´€í˜•": ["INFJ", "INFP", "ENFJ", "ENFP"],
    "ê´€ë¦¬ìí˜•": ["ISTJ", "ISFJ", "ESTJ", "ESFJ"],
    "íƒí—˜ê°€í˜•": ["ISTP", "ISFP", "ESTP", "ESFP"]
}

# Constants
MBTI_TYPES = [
    "INTJ", "INTP", "ENTJ", "ENTP",
    "INFJ", "INFP", "ENFJ", "ENFP",
    "ISTJ", "ISFJ", "ESTJ", "ESFJ",
    "ISTP", "ISFP", "ESTP", "ESFP"
]

MODEL_COLORS = {
    "gpt": "#10a37f",  # OpenAI ê·¸ë¦°
    "gemini": "#4285f4",  # Google ë¸”ë£¨
    "claude": "#8e44ad"  # Claude í¼í”Œ
}


# ê³„ì ˆ ìƒìˆ˜ ì¶”ê°€
SEASONS = {
    "ë´„": "spring",
    "ì—¬ë¦„": "summer",
    "ê°€ì„": "autumn",
    "ê²¨ìš¸": "winter"
}



@dataclass
class ScoringConfig:
    """í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    prompt: str
    criteria: List[str]
    min_score: int = 0
    max_score: int = 100
    
    def to_dict(self) -> dict:
        return {
            "prompt": self.prompt,
            "criteria": self.criteria,
            "min_score": self.min_score,
            "max_score": self.max_score
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'ScoringConfig':
        return cls(**data)


def create_adaptive_prompt(
    city_doc: str, 
    target_generation: str, 
    mbti: str = None,
    include_mbti: bool = False
) -> str:
    """ë¬¸ì„œ ê¸°ë°˜ ìœ ì—°í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    base_prompt = f"""
ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µë˜ëŠ” ë„ì‹œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ë§¤ë ¥ì ì¸ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì¹´í”¼ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

[ë„ì‹œ ì •ë³´]
{city_doc}

[ì¹´í”¼ ì‘ì„± ê°€ì´ë“œë¼ì¸]
1. ìœ„ ì •ë³´ëŠ” ì˜ê°ì„ ì–»ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.
2. ë„ì‹œì˜ í•µì‹¬ ë§¤ë ¥ì„ í¬ì°©í•´ ì‹ ì„ í•œ ê´€ì ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.
3. íƒ€ê²Ÿì¸µì— ë§ëŠ” í†¤ì•¤ë§¤ë„ˆë¥¼ ì‚¬ìš©í•˜ë˜, ì •ë³´ì˜ ë‚˜ì—´ì€ í”¼í•´ì£¼ì„¸ìš”.
4. ê°ì„±ì  ê³µê°ê³¼ êµ¬ì²´ì  íŠ¹ì§•ì´ ì¡°í™”ë¥¼ ì´ë£¨ë„ë¡ í•´ì£¼ì„¸ìš”.

[íƒ€ê²Ÿ ì •ë³´]
ì„¸ëŒ€: {target_generation}"""

    if include_mbti and mbti:
        mbti_prompt = f"""
MBTI: {mbti}
íŠ¹ë³„ ê³ ë ¤ì‚¬í•­: 
- {mbti} ì„±í–¥ì˜ ì—¬í–‰ ì„ í˜¸ë„ë¥¼ ë°˜ì˜
- í•´ë‹¹ ì„±í–¥ì˜ ê´€ì‹¬ì‚¬ì™€ ê°€ì¹˜ê´€ ê³ ë ¤"""
        base_prompt += mbti_prompt

    base_prompt += """

[ì œì•½ì‚¬í•­]
- í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ì´ëª¨ì§€ 1-2ê°œ í¬í•¨
- ë„ì‹œë§Œì˜ ë…íŠ¹í•œ íŠ¹ì§• í•˜ë‚˜ ì´ìƒ í¬í•¨
- í´ë¦¬ì…°ë‚˜ ì§„ë¶€í•œ í‘œí˜„ ì§€ì–‘
"""
    return base_prompt
# Load documents
def load_docs() -> Dict[str, Dict[str, str]]:
    docs_path = pathlib.Path("docs")
    docs = {
        "region": {},
        "generation": {},
        "mbti": {}
    }
    
    # Load region docs
    region_path = docs_path / "regions"
    if region_path.exists():
        for file in region_path.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                docs["region"][file.stem] = f.read()
    
    # Load generation docs
    generation_path = docs_path / "generations"
    if generation_path.exists():
        for file in generation_path.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                docs["generation"][file.stem] = f.read()
    
    # Load MBTI doc
    mbti_file = docs_path / "mbti" / "mbti_all.txt"
    if mbti_file.exists():
        with open(mbti_file, "r", encoding="utf-8") as f:
            content = f.read()
            # MBTI íŒŒì¼ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì €ì¥
            docs["mbti"]["mbti_all"] = content
    
    return docs



class AdCopyEvaluator:
    """ê´‘ê³  ì¹´í”¼ í‰ê°€ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, scoring_config: ScoringConfig):
        self.scoring_config = scoring_config
        self.results_cache = {}
    
    def evaluate(self, copy: str, model_name: str) -> Dict:
        """í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ íŒŒì‹±"""
        try:
            # ìºì‹œëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            cache_key = f"{copy}_{model_name}"
            if cache_key in self.results_cache:
                return self.results_cache[cache_key]
            
            # í‰ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            evaluation_prompt = f"""
{self.scoring_config.prompt}

í‰ê°€ ëŒ€ìƒ ì¹´í”¼: {copy}

í‰ê°€ ê¸°ì¤€:
{chr(10).join(f'- {criterion}' for criterion in self.scoring_config.criteria)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [í‰ê°€ ê·¼ê±°]
ìƒì„¸ì ìˆ˜: [ê° ê¸°ì¤€ë³„ ì ìˆ˜ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„]
"""
            # API calls by model
            if model_name == "gpt":
                #response = openai.ChatCompletion.create(
                response = client.chat.completions.create(
                    model=model_zoo[0],
                    messages=[{"role": "user", "content": evaluation_prompt}],
                    max_tokens=1000 
                )
                result_text = response.choices[0].message.content
            elif model_name == "gemini":
                try:
                    response = gemini_model.generate_content(prompt)
                    return response.text if hasattr(response, 'text') else "Gemini API ì‘ë‹µ ì˜¤ë¥˜"
                except Exception as e:
                    return f"Gemini í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            else:  # claude
                response = anthropic.messages.create(
                    model=model_zoo[2],
                    max_tokens=1000,
                    messages=[{"role": "user", "content": evaluation_prompt}]
                )
                result_text = response.content[0].text
            
            # Parse results
            parsed_result = self.parse_evaluation_result(result_text)
            
            # Cache results
            self.results_cache[cache_key] = parsed_result
            
            return parsed_result
            
        except Exception as e:
            st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "score": 0,
                "reason": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}",
                "detailed_scores": [0] * len(self.scoring_config.criteria)
            }

    def parse_evaluation_result(self, result_text: str) -> Dict:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        try:
            lines = result_text.split('\n')
            
            # ì ìˆ˜ ì¶”ì¶œ ê°œì„ 
            score_line = next(l for l in lines if 'ì ìˆ˜:' in l)
            # ìˆ«ìë§Œ ì¶”ì¶œí•˜ëŠ” ë¡œì§ ê°œì„ 
            score_text = ''.join(c for c in score_line.split('ì ìˆ˜:')[1] if c.isdigit() and c != '*')
            score = int(score_text) if score_text else 0
            
            # ì´ìœ  ì¶”ì¶œ
            reason_line = next(l for l in lines if 'ì´ìœ :' in l)
            reason = reason_line.split('ì´ìœ :')[1].strip()
            
            # ìƒì„¸ì ìˆ˜ ì¶”ì¶œ ê°œì„ 
            try:
                detailed_line = next(l for l in lines if 'ìƒì„¸ì ìˆ˜:' in l)
                detailed_scores_text = detailed_line.split('ìƒì„¸ì ìˆ˜:')[1].strip()
                detailed_scores = []
                
                for s in detailed_scores_text.split(','):
                    score_text = ''.join(c for c in s if c.isdigit() and c != '*')
                    detailed_scores.append(int(score_text) if score_text else 0)
            except:
                detailed_scores = [score] * len(self.scoring_config.criteria)
            
            return {
                "score": score,
                "reason": reason,
                "detailed_scores": detailed_scores[:len(self.scoring_config.criteria)]
            }
        except Exception as e:
            st.error(f"ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "score": 0,
                "reason": f"íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                "detailed_scores": [0] * len(self.scoring_config.criteria)
            }

def generate_copy(prompt: str, model_name: str) -> str:
    """ê´‘ê³  ì¹´í”¼ ìƒì„±"""
    try:
        if model_name == "gpt":
            #response = openai.ChatCompletion.create(
            response = client.chat.completions.create(
                model=model_zoo[0],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000
            )
            return response.choices[0].message.content.strip()
        elif model_name == "gemini":
            try:
                response = gemini_model.generate_content(prompt)
                return response.text if hasattr(response, 'text') else "Gemini API ì‘ë‹µ ì˜¤ë¥˜"
            except Exception as e:
                return f"Gemini í‰ê°€ ì‹¤íŒ¨: {str(e)}"
        else:  # claude
            response = anthropic.messages.create(
                model=model_zoo[2],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000
            )
            return response.content[0].text.strip()
    except Exception as e:
        return f"ìƒì„± ì‹¤íŒ¨: {str(e)}"
        
def visualize_evaluation_results(results: Dict):
    """ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜"""
    fig = go.Figure(data=go.Scatterpolar(
        r=results['detailed_scores'],
        theta=st.session_state.scoring_config.criteria,
        fill='toself',
        name='í‰ê°€ ì ìˆ˜'
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )
        ),
        showlegend=False,
        title="í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜"
    )
    return fig



def analyze_prompt_performance(history: List[dict]) -> dict:
    """í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¶„ì„"""
    if not history:
        return None
    
    latest = history[-1]
    prev = history[-2] if len(history) > 1 else None
    
    current_avg = sum(e['score'] for e in latest['evaluations'].values()) / 3
    prev_avg = sum(e['score'] for e in prev['evaluations'].values()) / 3 if prev else None
    
    analysis = {
        "current_score": current_avg,
        "improvement": current_avg - prev_avg if prev_avg else 0,
        "top_model": max(latest['evaluations'].items(), key=lambda x: x[1]['score'])[0],
        "suggestions": []
    }
    
    if analysis["improvement"] <= 0:
        analysis["suggestions"].append("ë” êµ¬ì²´ì ì¸ ì§€ì—­ íŠ¹ì§•ì„ ì–¸ê¸‰í•´ë³´ì„¸ìš”")
        analysis["suggestions"].append("íƒ€ê²Ÿì¸µì˜ ê´€ì‹¬ì‚¬ë¥¼ ë” ë°˜ì˜í•´ë³´ì„¸ìš”")
    
    return analysis

def create_performance_chart(history: List[dict]) -> go.Figure:
    """ì„±ëŠ¥ íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
    if not history:
        return None
        
    data = []
    for entry in history:
        timestamp = entry['timestamp']
        for model, eval_data in entry['evaluations'].items():
            data.append({
                'timestamp': timestamp,
                'model': model,
                'score': eval_data['score']
            })
    
    df = pd.DataFrame(data)
    
    fig = go.Figure()
    for model in ["gpt", "gemini", "claude"]:
        model_data = df[df['model'] == model]
        fig.add_trace(go.Scatter(
            x=model_data['timestamp'],
            y=model_data['score'],
            name=model.upper(),
            line=dict(color=MODEL_COLORS[model])
        ))
    
    fig.update_layout(
        title='í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ íŠ¸ë Œë“œ',
        xaxis_title="ì‹œê°„",
        yaxis_title="ì ìˆ˜",
        legend_title="ëª¨ë¸"
    )
    
    return fig



# Load documents
DOCS = load_docs()

# Initialize session state
if 'history' not in st.session_state:
    st.session_state.history = []
if 'show_tutorial' not in st.session_state:
    st.session_state.show_tutorial = True

# Initialize scoring config
DEFAULT_SCORING_CONFIG = ScoringConfig(
    prompt="""
ì£¼ì–´ì§„ ê´‘ê³  ì¹´í”¼ë¥¼ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
ê° ê¸°ì¤€ë³„ë¡œ 0-100ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³ , 
ìµœì¢… ì ìˆ˜ëŠ” ê° ê¸°ì¤€ì˜ í‰ê· ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """,
    criteria=[
        "íƒ€ê²Ÿ ì„¸ëŒ€ ì í•©ì„±",
        "ë©”ì‹œì§€ ì „ë‹¬ë ¥",
        "ì°½ì˜ì„±",
        "ì§€ì—­ íŠ¹ì„± ë°˜ì˜ë„"
    ]
)



if 'scoring_config' not in st.session_state:
    st.session_state.scoring_config = DEFAULT_SCORING_CONFIG
if 'evaluator' not in st.session_state:
    st.session_state.evaluator = AdCopyEvaluator(st.session_state.scoring_config)



# Tutorial
if st.session_state.show_tutorial:
    with st.sidebar:
        st.info("""
        ğŸ‘‹ ì²˜ìŒ ì˜¤ì…¨ë‚˜ìš”?
        
        1ï¸âƒ£ ì§€ì—­ê³¼ ì„¸ëŒ€ë¥¼ ì„ íƒí•˜ì„¸ìš”
        2ï¸âƒ£ ê³„ì ˆê³¼ MBTIë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
        3ï¸âƒ£ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ê²€í† /ìˆ˜ì •í•˜ì„¸ìš”
        4ï¸âƒ£ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì„¸ìš”
        
        ğŸ¯ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ë©° ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!
        """)
        if st.button("ì•Œê² ìŠµë‹ˆë‹¤!", use_container_width=True):
            st.session_state.show_tutorial = False

# Sidebar
with st.sidebar:
    # í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì • ë¶€ë¶„ ì¶”ê°€
    st.header("âš™ï¸ í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì •")
    
    with st.expander("í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
        new_prompt = st.text_area(
            "í‰ê°€ í”„ë¡¬í”„íŠ¸",
            value=st.session_state.scoring_config.prompt
        )
        new_criteria = st.text_area(
            "í‰ê°€ ê¸°ì¤€ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
            value="\n".join(st.session_state.scoring_config.criteria)
        )
        
        if st.button("í‰ê°€ ì„¤ì • ì—…ë°ì´íŠ¸"):
            new_config = ScoringConfig(
                prompt=new_prompt,
                criteria=[c.strip() for c in new_criteria.split('\n') if c.strip()]
            )
            st.session_state.scoring_config = new_config
            st.session_state.evaluator = AdCopyEvaluator(new_config)
            st.success("í‰ê°€ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.title("ğŸ¯ íƒ€ê²Ÿ ì„¤ì •")
    
    selected_region = st.selectbox(
        "ì§€ì—­ ì„ íƒ",
        options=[""] + list(DOCS["region"].keys()),
        format_func=lambda x: "ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )
    
    selected_generation = st.selectbox(
        "ì„¸ëŒ€ ì„ íƒ",
        options=[""] + list(DOCS["generation"].keys()),
        format_func=lambda x: "ì„¸ëŒ€ë¥¼ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )

    # ê³„ì ˆ ì„ íƒ ì¶”ê°€
    selected_season = st.selectbox(
        "ê³„ì ˆ ì„ íƒ (ì„ íƒì‚¬í•­)",
        options=[""] + list(SEASONS.keys()),
        format_func=lambda x: "ê³„ì ˆì„ ì„ íƒí•˜ì„¸ìš”" if x == "" else x
    )
    
    include_mbti = st.checkbox("MBTI íŠ¹ì„± í¬í•¨í•˜ê¸°")
    selected_mbti = None
    if include_mbti:
        selected_mbti = st.selectbox(
            "MBTI ì„ íƒ",
            options=MBTI_TYPES,
            help="ì„ íƒí•œ MBTI ì„±í–¥ì— ë§ëŠ” ì¹´í”¼ê°€ ìƒì„±ë©ë‹ˆë‹¤"
        )
# Main content
col1, col2 = st.columns([3, 2])

with col1:
    st.subheader("ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì‘ì„±")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = create_adaptive_prompt(
        city_doc=DOCS["region"].get(selected_region, "ì§€ì—­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."),
        target_generation=selected_generation,
        mbti=selected_mbti,
        include_mbti=include_mbti
    )
    
    # í”„ë¡¬í”„íŠ¸ í‘œì‹œ ë° í¸ì§‘ ê°€ëŠ¥í•˜ê²Œ
    edited_prompt = st.text_area(
        "ìƒì„± í”„ë¡¬í”„íŠ¸",
        value=prompt,
        height=400
    )
    
    if st.button("ğŸ¨ ê´‘ê³  ì¹´í”¼ ìƒì„±", use_container_width=True):
        if not selected_region or not selected_generation:
            st.error("ì§€ì—­ê³¼ ì„¸ëŒ€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner("AI ëª¨ë¸ì´ ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                # Generate copies
                results = {
                    model: generate_copy(prompt, model)
                    for model in ["gpt", "gemini", "claude"]
                }
                
                # Evaluate copies
                evaluations = {
                    model: st.session_state.evaluator.evaluate(copy, model)
                    for model, copy in results.items()
                }
                
                # Save results
                experiment_data = {
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "prompt": prompt,
                    "results": results,
                    "evaluations": evaluations,
                    "settings": {
                        "region": selected_region,
                        "generation": selected_generation,
                        "mbti": selected_mbti
                    }
                }
                st.session_state.history.append(experiment_data)
                
with col2:
    st.subheader("ì‹¤í—˜ ê²°ê³¼")
    
    if st.session_state.history:
        latest_experiment = st.session_state.history[-1]
        
        # ì„±ëŠ¥ ë¶„ì„
        analysis = analyze_prompt_performance(st.session_state.history)
        if analysis:
            try:
                st.markdown(f"""
                <div class="prompt-feedback">
                    <h4>ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„</h4>
                    <p>í˜„ì¬ í‰ê·  ì ìˆ˜: {analysis['current_score']:.1f}</p>
                    <p>ì´ì „ ëŒ€ë¹„: {analysis['improvement']:+.1f}</p>
                    <p>ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {analysis['top_model'].upper()}</p>
                    
                    <div class="improvement-tip">
                        ğŸ’¡ ê°œì„  í¬ì¸íŠ¸:
                        {'<br>'.join(f'- {s}' for s in analysis['suggestions'])}
                    </div>
                </div>
                """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"ì„±ëŠ¥ ë¶„ì„ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ê²°ê³¼ ì¹´ë“œ í‘œì‹œ
        model_list = ["gpt", "gemini", "claude"]
        for idx, model_name in enumerate(model_list):
            try:
                with st.container():
                    result = latest_experiment['results'].get(model_name, "ê²°ê³¼ ì—†ìŒ")
                    eval_data = latest_experiment['evaluations'].get(model_name, {
                        "score": 0,
                        "reason": "í‰ê°€ ì‹¤íŒ¨",
                        "detailed_scores": [0] * len(st.session_state.scoring_config.criteria)
                    })
                    
                    st.markdown(f"""
                    <div class="result-card">
                        <span class="model-tag" style="background-color: {MODEL_COLORS[model_name]}">
                            {model_name.upper()}
                        </span>
                        <div style="margin: 1rem 0;">
                            {result}
                        </div>
                        <div class="score-badge">
                            ì ìˆ˜: {eval_data.get('score', 0)}ì 
                        </div>
                        <div class="prompt-feedback">
                            {eval_data.get('reason', 'í‰ê°€ ì´ìœ  ì—†ìŒ')}
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if 'detailed_scores' in eval_data:
                        try:
                            fig = visualize_evaluation_results(eval_data)
                            st.plotly_chart(fig, use_container_width=True)
                        except Exception as e:
                            st.error(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            except Exception as e:
                st.error(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({model_name}): {str(e)}")

    else:
        st.info("ê´‘ê³  ì¹´í”¼ë¥¼ ìƒì„±í•˜ë©´ ì—¬ê¸°ì— ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
